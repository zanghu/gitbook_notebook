## lightGBM: 通过 pip 安装 GPU 版本

参考资料: [安装GPU版本的最新官方说明（含pip方式）](https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-version)


参考资料: [安装GPU版本的旧版官方说明（只包含源码编译方式）](https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html#lightgbm-gpu-tutorial)

参考资料：[最简便的lightGBM GPU支持的安装、验证方法](https://blog.csdn.net/lccever/article/details/80535058)

### 安装经验

实际安装命令（本机已经安装了`cuda 10.1`,`boost 1.67`）：

```shell
pip install lightgbm \
    --install-option=--gpu --install-option="--opencl-include-dir=/usr/local/cuda/include/" \
    --install-option="--opencl-library=/usr/local/cuda/lib64/libOpenCL.so" \
    --install-option="--boost-include-dir=/home/zanghu/ProgramFiles/software/boost/boost_1_67_0/include" \
    --install-option="--boost-librarydir=/home/zanghu/ProgramFiles/software/boost/boost_1_67_0/lib"
```

根据官方文档，可用的`--install-option`包括：

* boost-root
* boost-dir
* boost-include-dir
* boost-librarydir
* opencl-include-dir
* opencl-library

### 测试GPU

使用`HIGGS`（希格斯玻色子）数据集

#### STEP 0: 下载数据

```
git clone https://github.com/guolinke/boosting_tree_benchmarks.git
cd boosting_tree_benchmarks/data
wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz"
gunzip HIGGS.csv.gz
python higgs2libsvm.py
```

#### STEP 1: 测试代码

CPU版本

```python
# coding=utf-8
#!/usr/bin/env python
import lightgbm as lgb
import time
import os

def load_data():
    """"""
    dtrain = lgb.Dataset('/home/zanghu/data_base/HIGGS/higgs.train')
    return dtrain
     
def test_cpu(dtrain):
    """"""
    params = {'max_bin': 63,
    'num_leaves': 255,
    'learning_rate': 0.1,
    'tree_learner': 'serial',
    'task': 'train',
    'is_training_metric': 'false',
    'min_data_in_leaf': 1,
    'min_sum_hessian_in_leaf': 100,
    'ndcg_eval_at': [1,3,5,10],
    #'sparse_threshold': 1.0,
    'device': 'cpu'
    }
     
    t0 = time.time()
    gbm = lgb.train(params, train_set=dtrain, num_boost_round=1000,
              valid_sets=None, valid_names=None,
              fobj=None, feval=None, init_model=None,
              feature_name='auto', categorical_feature='auto',
              early_stopping_rounds=None, evals_result=None,
              verbose_eval=True,
              keep_training_booster=False, callbacks=None)
    t1 = time.time()
     
    print('cpu version elapse time: {}'.format(t1-t0))

if __name__ == '__main__':
    data = load_data()
    test_cpu(data)
```

GPU版本

```python
# coding=utf-8
#!/usr/bin/env python
import lightgbm as lgb
import time
import os

def load_data():
    """"""
    dtrain = lgb.Dataset('/home/zanghu/data_base/HIGGS/higgs.train')
    return dtrain

def test_gpu(dtrain):
    """"""
    params = {'max_bin': 63,
    'num_leaves': 255,
    'learning_rate': 0.1,
    'tree_learner': 'serial',
    'task': 'train',
    'is_training_metric': 'false',
    'min_data_in_leaf': 1,
    'min_sum_hessian_in_leaf': 100,
    'ndcg_eval_at': [1,3,5,10],
    #'sparse_threshold': 1.0,
    'device': 'gpu',
    'gpu_platform_id': 0,
    'gpu_device_id': 0}
     
    t0 = time.time()
    gbm = lgb.train(params, train_set=dtrain, num_boost_round=1000,
              valid_sets=None, valid_names=None,
              fobj=None, feval=None, init_model=None,
              feature_name='auto', categorical_feature='auto',
              early_stopping_rounds=None, evals_result=None,
              verbose_eval=True,
              keep_training_booster=False, callbacks=None)
    t1 = time.time()
     
    print('gpu version elapse time: {}s'.format(t1-t0))

if __name__ == '__main__':
    data = load_data()
    test_gpu(data)
```

#### STEP 2: 测试结果

确实是用了GPU，在1000轮迭代的情况下性能差异微乎其微

![](/assets/GBM001_01.PNG)

另外实际运行时用`top`和`nvidia-smi -l`观察，发现

* 不管是CPU还是GPU模式，CPU占用率都达到1200（12个逻辑核全部被占满）
* GPU模式下GPU平均负载很低，只有62W左右，温度42度

查了下资料，应该是lightGBM堆GPU的利用程度一直不是很高，数据频繁在CPU和GPU之间转移寻找分裂点，降低了GPU效率，参考资料：

[[GPU] further improving GPU performance #768](https://github.com/microsoft/LightGBM/issues/768#issuecomment-320573774)

[why running GPU version but the GPU-Util is 0% #3619](https://github.com/microsoft/LightGBM/issues/3619)